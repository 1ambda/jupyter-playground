{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고 문서\n",
    "- https://wikidocs.net/31766"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A barber is a person. a barber is good person. a barber is huge person. he '\n",
      " 'Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber '\n",
      " 'kept his word. a barber kept his word. His barber kept his secret. But '\n",
      " 'keeping and keeping such a huge secret to himself was driving the barber '\n",
      " 'crazy. the barber went up a huge mountain.')\n",
      "['A barber is a person.',\n",
      " 'a barber is good person.',\n",
      " 'a barber is huge person.',\n",
      " 'he Knew A Secret!',\n",
      " 'The Secret He Kept is huge secret.',\n",
      " 'Huge secret.',\n",
      " 'His barber kept his word.',\n",
      " 'a barber kept his word.',\n",
      " 'His barber kept his secret.',\n",
      " 'But keeping and keeping such a huge secret to himself was driving the barber '\n",
      " 'crazy.',\n",
      " 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "raw = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
    "pprint(raw)\n",
    "\n",
    "sentences = sent_tokenize(raw)\n",
    "pprint(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    doc = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        word = token.lower()\n",
    "        \n",
    "        if word not in stopwords_set and len(word) > 2:\n",
    "            doc.append(word)\n",
    "            \n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'],\n",
      " ['barber', 'good', 'person'],\n",
      " ['barber', 'huge', 'person'],\n",
      " ['knew', 'secret'],\n",
      " ['secret', 'kept', 'huge', 'secret'],\n",
      " ['huge', 'secret'],\n",
      " ['barber', 'kept', 'word'],\n",
      " ['barber', 'kept', 'word'],\n",
      " ['barber', 'kept', 'secret'],\n",
      " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
      " ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 케라스를 이용한 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocap_size = 20 # 상위 N 개 단어만 이용\n",
    "keras_tokenizer = Tokenizer(num_words = vocap_size + 1)\n",
    "keras_tokenizer.fit_on_texts(docs) # 빈도 기준으로 단어 집합을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1,\n",
      " 'crazy': 11,\n",
      " 'driving': 10,\n",
      " 'good': 8,\n",
      " 'huge': 3,\n",
      " 'keeping': 7,\n",
      " 'kept': 4,\n",
      " 'knew': 9,\n",
      " 'mountain': 13,\n",
      " 'person': 5,\n",
      " 'secret': 2,\n",
      " 'went': 12,\n",
      " 'word': 6}\n"
     ]
    }
   ],
   "source": [
    "pprint(keras_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8),\n",
      "             ('person', 3),\n",
      "             ('good', 1),\n",
      "             ('huge', 5),\n",
      "             ('knew', 1),\n",
      "             ('secret', 6),\n",
      "             ('kept', 4),\n",
      "             ('word', 2),\n",
      "             ('keeping', 2),\n",
      "             ('driving', 1),\n",
      "             ('crazy', 1),\n",
      "             ('went', 1),\n",
      "             ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "pprint(keras_tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5],\n",
      " [1, 8, 5],\n",
      " [1, 3, 5],\n",
      " [9, 2],\n",
      " [2, 4, 3, 2],\n",
      " [3, 2],\n",
      " [1, 4, 6],\n",
      " [1, 4, 6],\n",
      " [1, 4, 2],\n",
      " [7, 7, 3, 2, 10, 1, 11],\n",
      " [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "pprint(keras_tokenizer.texts_to_sequences(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5],\n",
      " [1, 8, 5],\n",
      " [1, 3, 5],\n",
      " [9, 2],\n",
      " [2, 4, 3, 2],\n",
      " [3, 2],\n",
      " [1, 4, 6],\n",
      " [1, 4, 6],\n",
      " [1, 4, 2],\n",
      " [7, 7, 3, 2, 10, 1, 11],\n",
      " [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "word_indice = keras_tokenizer.word_index\n",
    "word_sequences = keras_tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "pprint(word_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(word_sequences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원-핫 인코딩의 한계\n",
    "\n",
    "- 단어의 개수가 늘어날수록 벡터를 위한 공간 (메모리) 가 많이 필요 = Sparse\n",
    "- 또한 단어간 유사도를 표현하지 못함.\n",
    "\n",
    "이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여, 다차원 공간에 벡터화 하는 기법으로 크게 두 가지가 존재\n",
    "\n",
    "1. 카운트 기반의 벡터화 방법인 LSA, HAL\n",
    "2. 예측 기반으로 벡터화 하는 NNLM, RNNLM, Word2Vec, FastText 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
